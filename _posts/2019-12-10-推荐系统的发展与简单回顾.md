---
layout:     post
title:      推荐系统的发展与简单回顾
subtitle:   表示学习、深度学习、强化学习、知识图谱、多任务学习
date:       2019-12-10
author:     Jiayue Cai
header-img: img/post-bg-debug.png
catalog: true
tags:
    - Recommending System
---

> Last updated on 2020-11-1... 

结合百度和支付宝两段推荐系统相关的实习经历，本篇针对 **工业界的模型发展** 做了简单梳理与回顾。

`表示学习`和`深度学习`在推荐系统中的应用是目前工业界比较成熟的，但是与`强化学习`、`知识图谱`、`多任务学习`相结合是比较少的，
一方面此类技术与推荐结合才刚刚开始探索，背后有太多问题需要人力去挖掘和探索；另一方面在公司的业务中敢不敢上这种前沿课题的探索与实验甚至上线接大流量，部门老大的`魄力`很关键。

> 本篇之外，部分知名外企发表的相关应用论文整理在[《Airbnb实时搜索排序》](https://coladrill.github.io/2019/05/19/Airbnb%E5%AE%9E%E6%97%B6%E6%90%9C%E7%B4%A2%E6%8E%92%E5%BA%8F/)的文末。

### 表示学习

常用类别特征的表示方法：One-Hot Encoding、Look-Up Embedding、Pre-Train Embedding

#### 框架

> [美团王永康前辈的分享](https://blog.csdn.net/gamer_gyt/article/details/97985818)

![](/img/post/20191210/1.png)

#### 序列 Sequence

![](/img/post/20191210/2.png)

#### 图 Graph

![](/img/post/20191210/3.png)

#### 附加信息 Side Information

Side Information框架是解决冷启动的方法之一。

![](/img/post/20191210/4.png)

#### 多模态 Multimodal

![](/img/post/20191210/5.png)

更准确来说，这部分属于模式识别范畴，各大公司在业务稳定后基本都会成立单独的内容理解团队，以分享到外部的资料举例，优酷团队在这方面做的很不错：[《优酷在多模态内容理解上的研究及应用》](https://www.infoq.cn/article/xgP_eyfidAA2l5ShcCPp)。

![](/img/post/20191210/12.png)


### 深度学习

![](/img/post/20191210/7.png)

企业级的推荐系统为了尽量提高模型的准确性，往往会使用丰富的甚至异构的内容数据。这些特征从不同的维度展现了不同的信息，而且特征间的组合通常是非常有意义的。传统的交叉特征是由工程师手动设计的，这有很大的局限性，成本很高，并且不能拓展到未曾出现过的交叉模式中。因此学者们开始研究用神经网络去自动学习高阶的特征交互模式，弥补人工特征工程带来的种种局限性。

> [基于深度学习的推荐系统综述](https://coladrill.github.io/2018/08/08/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/)、[浅梦的Github链接](https://github.com/shenweichen/DeepCTR)、[最新论文导读](https://zhuanlan.zhihu.com/weichennote?utm_source=qq&utm_medium=social&utm_oi=566394839504048128)

![](/img/post/20191210/8.png)

#### 2015-2016

|       **CCPM**       | `[CIKM 2015]`[A Convolutional Click Prediction Model](http://ir.ia.ac.cn/bitstream/173211/12337/1/A%20Convolutional%20Click%20Prediction%20Model.pdf)             |
|       **FNN**        | `[ECIR 2016]`[Deep Learning over Multi-field Categorical Data: A Case Study on User Response Prediction](https://arxiv.org/pdf/1601.02376.pdf)                    |
|       **PNN**        | `[ICDM 2016]`[Product-based neural networks for user response prediction](https://arxiv.org/pdf/1611.00144.pdf)                                                   |
|   **Wide & Deep**    | `[DLRS 2016]`[Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf)                                                                 |

#### 2017-2018

|      **DeepFM**      | `[IJCAI 2017]`[DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](http://www.ijcai.org/proceedings/2017/0239.pdf)                           |
|       **PWLM**       | `[arxiv 2017]`[Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction](https://arxiv.org/abs/1704.05194)                                 |
|       **DCN**        | `[ADKDD 2017]`[Deep & Cross Network for Ad Click Predictions](https://arxiv.org/abs/1708.05123)                                                                   |
|       **AFM**        | `[IJCAI 2017]`[Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks](http://www.ijcai.org/proceedings/2017/435) |
|       **NFM**        | `[SIGIR 2017]`[Neural Factorization Machines for Sparse Predictive Analytics](https://arxiv.org/pdf/1708.05027.pdf)                                               |
|     **xDeepFM**      | `[KDD 2018]`[xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems](https://arxiv.org/pdf/1803.05170.pdf)                         |

#### 2018-2019

|       **DIN**        | `[KDD 2018]`[Deep Interest Network for Click-Through Rate Prediction](https://arxiv.org/pdf/1706.06978.pdf)                                                       |
|       **DIEN**       | `[AAAI 2019]`[Deep Interest Evolution Network for Click-Through Rate Prediction](https://arxiv.org/pdf/1809.03672.pdf)                                            |
|     **AutoInt**      | `[CIKM 2019]`[AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks](https://arxiv.org/abs/1810.11921)                              |
|       **NFFM**       | `[arxiv 2019]`[Operation-aware Neural Networks for User Response Prediction](https://arxiv.org/pdf/1904.12579.pdf)                                                |
|      **FGCNN**       | `[WWW 2019]`[Feature Generation by Convolutional Neural Network for Click-Through Rate Prediction ](https://arxiv.org/pdf/1904.04447)                             |
|       **DSIN**       | `[IJCAI 2019]`[Deep Session Interest Network for Click-Through Rate Prediction ](https://arxiv.org/abs/1905.06482)                                                |
|     **FiBiNET**      | `[RecSys 2019]`[FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction](https://arxiv.org/pdf/1905.09433.pdf)   |

> 自动特征工程方面我即将出一篇论文，期待下~


### 强化学习

> [RecSys2019参会总结](https://zhuanlan.zhihu.com/p/87381685)、[61篇NIPS2019顶会深度强化学习论文汇总与部分解读](https://cloud.tencent.com/developer/article/1505352)、[强化学习路线图](https://zhuanlan.zhihu.com/p/104224859?utm_source=qq&utm_medium=social&utm_oi=566394839504048128)

用户与推荐系统之间往往会发生持续密切的交互行为，强化学习中的Reward机制十分适合应用到这种模式，所以基于强化学习的推荐方法中，往往会**把推荐系统看做智能体（Agent）、把用户看做环境（Environment），商品的推荐问题可以看做经典的顺序决策问题。**Agent每一次排序策略的选择可以看做一次试错（Trial and Error），把用户的反馈、点击成交等作为从环境中获得的奖赏（Exploration & Exploitation）。

在支付宝实习的时候，优化CTR模型之余，针对商家线下运营策略推荐场景，主导实现了off-policy的强化学习模型。（下图仅作抛砖引玉，模型细节不能公布哈）

![](/img/post/20191210/9.png)

![](/img/post/20191210/10.png)

#### 2018-2019

|`[ICDM 2018]`[Self-attentive sequential recommendation](https://arxiv.org/pdf/1808.09781.pdf)                                                                               |
|`[arXiv 2018]`[Reinforcement Learning for Online Information Seeking](https://arxiv.org/pdf/1812.07127.pdf)                                                                 |
|`[WWW 2018]`[DRN:A Deep Reinforcement Learning Framework for News Recommendation](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf) |                                                                                                               |
|`[SIGIR 2019]`[Lifelong Sequential Modeling with Personalized Memorization for User Response Prediction](https://arxiv.org/pdf/1905.00758.pdf)                              |
|`[ICML 2019]`[Generative Adversarial User Model for Reinforcement Learning Based Recommendation System](http://proceedings.mlr.press/v97/chen19f/chen19f.pdf)               |
|`[AAAI 2019]`[Large-scale Interactive Recommendation with Tree-structured Policy Gradient](https://www.aaai.org/ojs/index.php/AAAI/article/view/4204/4082)                  |
|`[arXiv 2019]`[Reinforcement Learning to Optimize Long-term User Engagement in Recommender Systems](https://arxiv.org/pdf/1902.05570.pdf)                                   |
|`[WSDM 2019]`[Top-K Off-Policy Correction for a REINFORCE Recommender System](https://arxiv.org/pdf/1812.02353.pdf)                                                         |
|`[arXiv 2019]`[Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology](https://arxiv.org/pdf/1905.12767.pdf)       |

#### 机遇与挑战

1. **离线评估与在线效果之间的巨大鸿沟**
- 离线模拟过程的泛化性问题，特别是用户端连续决策行为的建模方面。
- 现有方法其实还是有很多问题，例如没有刻画用户反馈行为的连续性与前后相关性、没有考虑用户反馈背后的多因素影响等等。

2. **动作空间偏大**
- 在真实的推荐系统中面临巨大无比的离散化行为空间（待推荐item集合巨大无比）。
- [TPGR模型](https://www.aaai.org/ojs/index.php/AAAI/article/view/4204/4082)在这个问题上已经作出了一些探索，但还没有完全解决这类问题。

3. **在线效果容易受到其它策略的影响**
- 例如推荐端在使用RL算法，用户还在看到很多其它推荐、展示策略在其它位置、时段、平台推给他的结果，较难精确建模用户反馈与行为归因。

4. **如何去做更好地探索**
- 因为真实场景中没有游戏环境那样丰富的样本数据，每一次探索都有巨大的显性成本或隐性成本。
- 更快地探索、更好地采用效率是RL4Rec实际应用中的难题。

> [炼丹感悟：On the Generalization of RL](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247503049&idx=2&sn=1164dcccb7cede8d75a743f9739b1c0f&chksm=96ea1349a19d9a5f792e91088096c2ecbb2dac7a00b6c373eb9848bd87234c97996cbaf89a5f&mpshare=1&scene=23&srcid=0212S6vjRE8w0vvZdvG3o1Zo&sharer_sharetime=1581473512553&sharer_shareid=cc983be31429dfbd5199d63f0d94b825#rd)

### 知识图谱

> [Personalized Recommendation Systems: Five Hot Research Topics You Must Know](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/personalized-recommendation-systems/)

在多数推荐场景中，物品可能包含丰富的知识信息，而刻画这些知识的网络结构即被称为知识图谱。物品端的知识图谱极大地扩展了物品的信息，强化了物品之间的联系，为推荐提供了丰富的参考价值，更能为推荐结果带来额外的`多样性`和`可解释性`。

![](/img/post/20191210/11.png)

和社交网络相比，知识图谱是一种异构网络（一般由三元组 `< 头节点，关系，尾节点 >` 组成），因此针对知识图谱的推荐算法设计要更复杂和精巧。
引入网络特征学习的方法处理推荐系统中知识图谱的相关信息，有助于增强推荐系统的学习能力，提高精确度和用户满意度。

**将知识图谱引入推荐系统，主要有以下两种不同的处理方式：**
- **基于特征的辅助推荐模型：**核心是知识图谱特征学习的引入。即首先使用知识图谱特征学习对其进行处理，从而得到实体和关系的低维稠密向量表示。这些低维的向量表示可以较为自然地与推荐系统进行结合和交互。
- **基于结构的全局推荐模型：**更加直接地使用知识图谱的结构特征。具体来说，对于知识图谱中的每一个实体，我们都进行BFS来获取其在知识图谱中的多跳关联实体从中得到推荐结果。

#### 2018-2019

|`[KDD 2018]`[Leveraging Meta-path based Context for Top-N Recommendation with A Neural Co-Attention Model](http://shichuan.org/doc/47.pdf)              |
|`[WWW 2019]`[Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation](https://arxiv.org/pdf/1901.08907)                                 |
|`[WWW 2019]`[Knowledge Graph Convolutional Networks for Recommender Systems](https://arxiv.org/pdf/1905.04413)                                          |
|`[KDD 2019]`[KGAT: Knowledge Graph Attention Network for Recommendation](https://arxiv.org/pdf/1905.07854)                                              |
|`[KDD 2019]`[Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems](https://arxiv.org/pdf/1905.04413.pdf)  |
|`[DLP-KDD 2019]`[An End-to-End Neighborhood-based Interaction Model for Knowledge-enhanced Recommendation](https://arxiv.org/pdf/1908.04032.pdf)        |
|`[AAAI 2019]`[Explainable Reasoning over Knowledge Graphs for Recommendation](https://arxiv.org/pdf/1811.04540.pdf)                                     |
|`[SIGIR 2019]`[Reinforcement Knowledge Graph Reasoning for Explainable Recommendation](https://arxiv.org/pdf/1906.05237.pdf)                            |

#### 机遇与挑战

1. **现有模型都属于统计学习模型，即挖掘网络中的统计学信息并以此进行推断**
- 一个困难但更有研究前景的方向是在网络中进行推理，将`图推理`与推荐系统相结合。

2. **如何设计出性能优秀且运行效率高的算法，也是潜在的研究方向**
- 现有模型并不涉及`计算引擎层面`、`系统层面`甚至`硬件层面`的考量，如何将上层算法和底层架构进行联合设计和优化，是实际应用中一个亟待研究的问题。

3. **现有的模型网络结构都是静态的，在真实场景中，知识图谱具有一定的时效**
- 如何刻画这种时间演变的网络，并在推荐时充分`考虑时序信息`，也值得我们未来研究。

在工业界中，一方面构建一张图需要花费巨大的人力；另一方面图采样等相关技术还不成熟（暴力地使用GCN、KGAT并不现实）。
据目前我所了解到的，在知识图谱工业级应用方面，谷歌和百度是比较前沿的（重视程度源于索引量大且对query质量要求高时的场景需求）。

> [百度大规模知识图谱构建和应用](https://mp.weixin.qq.com/s?__biz=MzU1NDA4NjU2MA==&mid=2247500478&idx=2&sn=ed0a9c12d9fa393be8f0453f89fa15df&chksm=fbea7b71cc9df2674c5f9be2a2b324747949f4cc353f2a19f5649ded148f2279afd7c12a2e4e&mpshare=1&scene=23&srcid=&sharer_sharetime=1582350922986&sharer_shareid=cc983be31429dfbd5199d63f0d94b825#rd)、[明略科技张杰：将知识图谱引入数据仓库](https://mp.weixin.qq.com/s?__biz=MzI5NTIxNTg0OA==&mid=2247499878&idx=1&sn=60f7671e4ef5ea263b984239a0e3aba6&chksm=ec5477e1db23fef7ec7eb6e433aba0115554a496f15378166f5897b799eabb49a63215b14333&mpshare=1&scene=23&srcid=&sharer_sharetime=1576548799665&sharer_shareid=cc983be31429dfbd5199d63f0d94b825#rd)

### 多任务学习

> [《多任务学习在推荐算法中的应用》](https://mp.weixin.qq.com/s?__biz=MzU2ODA0NTUyOQ==&mid=2247484352&idx=1&sn=2000215d391698fe5c7acbc2d7b4b077&chksm=fc92b976cbe5306070a05d47255f2450c13aa6a26d21573ab990d66daf6829fb84f9b82f45ef&mpshare=1&scene=23&srcid=&sharer_sharetime=1577885490293&sharer_shareid=cc983be31429dfbd5199d63f0d94b825#rd)、[imsheridan的Github链接](https://github.com/imsheridan/DeepRec)、[Alex-zhai的知乎](https://www.zhihu.com/people/alex-zhai-19/posts)、[卢明冬的博客](https://lumingdong.cn/multi-task-learning-in-recommendation-system.html)

我们在优化推荐效果的时候，很多时候不仅仅需要关注 CTR 指标，同时还需要优化例如 CVR ( 转化率 )、视频播放时长、用户停留时长、用户翻页深度、关注率、点赞率这些指标。
那么一种做法是对每个任务单独使用一个模型来优化，但是这样做的缺点显而易见，需要花费很多人力。其实很多任务之间都是存在`关联性的`，比如 CTR 和 CVR。
那么能不能使用一个模型来同时优化两个或多个任务呢？其实这就是 Multi-task 多任务的定义。

具体一点地说，当我们完成`召回阶段`之后，有了可供推荐的候选 Item，然后从数据库中获取 User 和 Item 特征（特征缓存），从参数服务器获取模型参数（权重缓存），就可以进行预估。
这里假定淘宝有五个预估目标，分别是点击率 CTR、购买转化率 CVR、收藏率 collect，加购率 cart、停留时长 stay，这五个目标分别对应五个模型，`粗排阶段`的作用就是利用模型根据各自目标来给候选 Item 计算一个预估值（分数），排序阶段结束每个 Item 都会有五个不同的目标预估分数，如何用这些分数，是交给下一个流程来处理的。
`精排阶段`有两个主要作用，一个是融合，也就是将排序阶段传递来的多个目标分数进行融合，形成一个分数，另外一个是约束，也就是规制过滤。

#### 2018-2019

|     **ESMM**      |`[SIGIR 2018]`[Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate](https://arxiv.org/pdf/1804.07931.pdf)                   |
|     **DUPN**      |`[KDD 2018]`[Perceive Your Users in Depth: Learning Universal User Representations from Multiple E-commerce Tasks](https://arxiv.org/pdf/1805.10727.pdf)               |
|     **MMoE**      |`[KDD 2018]`[Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007?download=true)  |
|     **ESM2**      |`[arXiv 2019]`[Conversion Rate Prediction via Post-Click Behaviour Modeling](https://arxiv.org/pdf/1910.07099.pdf)  |
|     **MMoE2**     |`[RecSys 2019]`[Recommending what video to watch next: A multitask ranking system](https://daiwk.github.io/assets/youtube-multitask.pdf)                               |

> 以上部分论文是有理论缺陷的，需仔细甄别。

#### 总结与思考

多任务学习的定义中，**有两个非常关键的限定**，也是多任务得以实现的前提条件：多个任务之间必须具有`相关性`以及拥有可以`共享的底层表示`。（当多个任务的相关性没那么强时，这些任务之间就会相互扰乱，从而影响最后的效果）

**共享表示对于最终任务的学习有两类作用：**
- **促进作用：**通过浅层的共享表示互相分享、互相补充学习到的领域相关信息，从而互相促进学习，提升对信息的穿透和获取能力；
- **约束作用：**在多个任务同时进行反向传播时，共享表示则会兼顾到多个任务的反馈，由于不同的任务具有不同的噪声模式，所以同时学习多个任务的模型就会通过平均噪声模式从而学习到更一般的表征。（这个有点像正则化的意思，因此相对于单任务，过拟合风险会降低，泛化能力增强）

**执行多任务学习的两种最常用的方法：**
- **参数的硬共享机制（基于参数的共享，Parameter Based）**

共享 Hard 参数是神经网络 MTL 最常用的方法。在实际应用中，通常通过在所有任务之间共享隐藏层，同时保留几个特定任务的输出层来实现，如下图所示：
![](/img/post/20191210/13.png)
这种方法大大降低了过拟合的风险。这很直观：我们同时学习的工作越多，我们的模型找到一个含有所有任务的表征就越困难，而过拟合我们原始任务的可能性就越小。

- **参数的软共享机制（基于约束的共享，Regularization Based）**

即每个任务都有自己的参数和模型。模型参数之间的距离是正则化的，以便鼓励参数相似化，例如使用 L2 距离进行正则化。
![](/img/post/20191210/14.png)
约束深度神经网络 Soft 参数共享的思想受到了 MTL 正则化技术的极大启发，这种思想已经用于其它模型开发。

具体如何设计 MTL 的结构，首先在基于 MTL 基本原则情况下（存在共享表示和相关任务），应从任务特点、资源消耗、性能效果等方面去综合考虑。

### 结束语

model是实验室的产物，solution才是推向市场的结果，毕竟客户要的是solution，而不是model。
solution意味着需要`产品化`和`工程化`的思维方式，这是工业界和学术界最大的不同，也是其魅力所在。

> [CIKM 2020 搜索推荐广告论文集锦](https://mp.weixin.qq.com/s?__biz=MzkzNjEyNTc4NA==&mid=2247483864&idx=1&sn=a9d40556302c1cd281641567c8ffe60d&chksm=c2a235eff5d5bcf9f1004dd33d27e424021174314cda996a8f842babfcbfbae40270fd1bfd66&mpshare=1&scene=23&srcid=1029wSs2DuiXSiOgWZ8KWkEc&sharer_sharetime=1604210297267&sharer_shareid=cc983be31429dfbd5199d63f0d94b825#rd)



