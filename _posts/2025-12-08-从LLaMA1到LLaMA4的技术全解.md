---
layout:     post
title:      从LLaMA1到LLaMA4的技术全解
subtitle:   LLaMA、架构、算法
date:       2025-12-08
author:     Jiayue Cai
header-img: img/post-bg-ai.jpg
catalog: true
tags:
    - LLM
    - AI
---

> Last updated on 2025-12-8... 

### 模型发布概览

> [From Llama 1 to Llama 4: Meta’s AI Evolution Unleashed](https://www.linkedin.com/pulse/from-llama-1-4-metas-ai-evolution-unleashed-dheeraj-kumar-k1vxc/)

| 版本           | LLaMA 1                  | LLaMA 2                          | LLaMA 3 / 3.1                     | LLaMA 4（2025年4月5日发布）                                      |
|----------------|--------------------------|----------------------------------|-----------------------------------|------------------------------------------------------------------|
| 发布日期       | 2023年2月                | 2023年7月                        | 2024年4月（3.1为7月更新）          | 2025年4月5日                                                     |
| 参数规模       | 7B、13B、33B、65B        | 7B、13B、70B（含Chat版）          | 8B、70B、405B                     | Scout：17B活跃/109B总<br>Maverick：17B活跃/400B总<br>Behemoth：288B活跃/约2T总（即将发布） |
| 训练数据量     | 1.4万亿 token（公开数据） | 2万亿 token（更高质量、多样）     | 15万亿 token（含大量代码+30+语言）  | 30万亿 token（首次加入Meta内部用户数据）                           |
| 上下文长度     | 未特别强调               | 4K token                         | 128K（部分变体支持1M）             | Scout：1000万 token<br>Maverick：100万 token                      |
| 主要基准表现   | 13B在多项任务超越GPT-3（175B） | 接近或超过同期闭源模型            | 大幅领先同级别开源模型，逼近GPT-4   | Maverick在多语言、编码任务超越GPT-4o<br>Scout长上下文表现极强      |
| 关键技术与特性 | 纯研究用途、高效证明概念  | 开源权重+商业许可、RLHF安全对齐   | 多语言（30+种）、超长上下文、驱动Meta全家桶产品 | MoE稀疏激活极致高效<br>原生多模态（图文输入→文本输出）<br>支持12种语言（数据量提升10倍） |
| 许可与可用性   | 仅限研究，禁止微调       | 开源可商用                       | 开源可商用                        | 开源可商用（最宽松版本）                                          |


### LLaMA1

LLaMA1是 Meta 开发的一个高效开源语言模型，在继承 Transformer 架构的基础上，LLaMA1 进行了一系列关键的技术改进，包括`预归一化与RMSNorm`、`旋转位置编码(ROPE) ` 、`SwiGLU`激活函数等技术，这些改进显著提升了模型的性能和训练稳定性。

#### 预归一化与RMSNorm

> [The Evolution of Llama: From Llama 1 to Llama 3.1](https://zaai.ai/the-evolution-of-llama-from-llama-1-to-llama-3-1/)

受 GPT3 架构中训练稳定性改进的启发 [4]，Llama 1 也对每个 Transformer 子层的输入进行归一化，而不仅仅是对输出进行归一化，如下图所示：

![](/img/post/20251208/1.png)

此外，使用 RMSNorm 代替了传统的 LayerNorm 函数，通过归一化向量的均方根值，而不需要中心化或依赖均值。

![](/img/post/20251208/2.png)

RMSNorm之所以能取得更高的效率，是因为其作者证明了LayerNorm的优势源于重缩放不变性而非中心化不变性。这一发现使他们能够从归一化过程中移除均值计算，从而简化了流程，使其同样有效，并显著提高了效率。

#### 旋转位置编码(ROPE)

**位置嵌入**对于语言学习模型（LLM）至关重要，因为Transformer架构具有顺序不变性。这意味着即使两个句子使用了相同的词语，但顺序不同且含义不同，Transformer也会以相同的方式表示它们。例如，如果不应用位置嵌入，以下句子对于Transformer来说含义相同：

- Sentence 1: Llama 2 is better than Llama 1 
- Sentence 2: Llama 1 is better than Llama 2

原始论文（Attention Is All You Need）实现了绝对位置嵌入，该嵌入通过两个正弦函数（正弦和余弦）表示。序列中的每个位置都有一个唯一的位置嵌入，这些位置嵌入被加总到词嵌入中，从而确保包含相同词语的两个句子含义不同。

![](/img/post/20251208/3.png)

尽管它已经解决了Transformer模型顺序不变的问题，但它仍然创建彼此独立的嵌入位置。其结果是，两个位置之间的邻近性没有被建模。这意味着从模型的角度来看，位置1和2与位置1和500之间的相关性没有区别。我们知道实际情况并非如此，因为理论上，位置1和2中单词的相似度应该高于位置1和500中单词的相似度。

**旋转位置嵌入(RoPE)**可以解决这个问题，它通过旋转词嵌入来表示序列中每个位置，从而对单词的相对位置进行建模。我们沿用之前的例子：“Llama 2 is better than Llama 1”，并假设词嵌入现在是二维的。单词“better”将由原始二维向量基于其位置 m(4) 和常数 θ 旋转得到的二维向量表示，如下图所示：

![](/img/post/20251208/4.png)

这种方法可以让我们保持词语之间的相对距离，因为即使我们在原句中添加更多词语，两个向量之间的相似度也保持不变。例如，假设我们在句子“LLM Llama 2 比 Llama 1 好”中添加两个词，其中“better”和“than”的位置不同（4 和 5 → 6 和 7），但由于旋转角度相同，两个向量之间的相似度也保持不变（左图中向量之间的点积与右图中的点积相同）。

![](/img/post/20251208/5.png)

![RoPE embeddings](https://x.com/rasbt/status/1637803707793326081/photo/1)

![](/img/post/20251208/6.png)

#### SwiGLU 激活函数

**SwiGLU（Swish-Gated Linear Unit）**是一种结合非线性激活函数的门控单元激活机制，相比传统的 ReLU 函数，增强了激活函数的表达能力，使模型可以捕捉更复杂的模式。此外，SwiGLU 减少了梯度消失问题(ReLU 容易导致负值被切为0)，从而提升训练稳定性和模型性能。

**1. 基本定义**

给定输入向量 ( x )，线性层分别生成两个投影：

* ( xW_1 )
* ( xW_2 )

SwiGLU 的定义如下：

```markdown
**SwiGLU(x) = (xW₁) ⊗ swish(xW₂)**
```

其中：

* ( ⊗ ) 表示逐元素乘法（element-wise product）
* Swish 激活函数定义为：

[
\text{swish}(z) = z \cdot \sigma(z)
]

* Sigmoid：

[
\sigma(z) = \frac{1}{1 + e^{-z}}
]

---

**2. 详细公式展开**

[
\text{SwiGLU}(x) = (xW_1) \cdot \big[(xW_2) \cdot \sigma(xW_2)\big]
]

**3. 与其他 GLU 变体的对比**

| 激活名称       | 公式                              | 激活函数类型  |
| ---------- | ------------------------------- | ------- |
| **GLU**    | ( (xW_1) ⊗ \sigma(xW_2) )       | Sigmoid |
| **ReGLU**  | ( (xW_1) ⊗ \text{ReLU}(xW_2) )  | ReLU    |
| **GEGLU**  | ( (xW_1) ⊗ \text{GELU}(xW_2) )  | GELU    |
| **SwiGLU** | ( (xW_1) ⊗ \text{swish}(xW_2) ) | Swish   |

SwiGLU 在性能与计算效率上获得更好折中，是 LLaMA、PaLM 等模型的默认 MLP 激活。

![](/img/post/20251208/7.png)

SwiGLU 具有一个可训练参数 β，用于控制插值程度。如图 4 所示，随着 β 值的增加，其行为越来越类似于 ReLU（当 β=100 时，ReLU 和 SwiGLU 重叠）。

### LLaMA2

LLaMA2 的架构和 LLaMA1 没有太大的变化，只是多了`分组查询注意力(GQA)`。而且在上下文处理能力、注意力机制优化、训练数据质量、性能表现、多语言支持和推理效率方面实现了全面改进，进一步缩小了开源模型与闭源大型模型之间的性能差距。

#### 分组查询注意力(GQA)






### LLaMA3



### LLaMA4