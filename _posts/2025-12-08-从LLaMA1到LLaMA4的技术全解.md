---
layout:     post
title:      从LLaMA1到LLaMA4的技术全解
subtitle:   LLaMA、算法演进
date:       2025-12-08
author:     Jiayue Cai
header-img: img/post-bg-ai.jpg
catalog: true
tags:
    - LLM
    - AI
---

> Last updated on 2025-12-8... 

### 模型发布概览

> [From Llama 1 to Llama 4: Meta’s AI Evolution Unleashed](https://www.linkedin.com/pulse/from-llama-1-4-metas-ai-evolution-unleashed-dheeraj-kumar-k1vxc/)

| 版本           | LLaMA 1                  | LLaMA 2                          | LLaMA 3 / 3.1                     | LLaMA 4（2025年4月5日发布）                                      |
|----------------|--------------------------|----------------------------------|-----------------------------------|------------------------------------------------------------------|
| 发布日期       | 2023年2月                | 2023年7月                        | 2024年4月（3.1为7月更新）          | 2025年4月5日                                                     |
| 参数规模       | 7B、13B、33B、65B        | 7B、13B、70B（含Chat版）          | 8B、70B、405B                     | Scout：17B活跃/109B总<br>Maverick：17B活跃/400B总<br>Behemoth：288B活跃/约2T总（即将发布） |
| 训练数据量     | 1.4万亿 token（公开数据） | 2万亿 token（更高质量、多样）     | 15万亿 token（含大量代码+30+语言）  | 30万亿 token（首次加入Meta内部用户数据）                           |
| 上下文长度     | 2K token               | 4K token                         | 128K（部分变体支持1M）             | Scout：1000万 token<br>Maverick：100万 token                      |
| 词表大小     | 32K               | 32K                        | 128K            | 128K                      |
| 主要基准表现   | 13B在多项任务超越GPT-3（175B） | 接近或超过同期闭源模型            | 大幅领先同级别开源模型，逼近GPT-4   | Maverick在多语言、编码任务超越GPT-4o<br>Scout长上下文表现极强      |
| 关键技术与特性 | 纯研究用途、高效证明概念  | 开源权重+商业许可、RLHF安全对齐   | 多语言（30+种）、超长上下文、驱动Meta全家桶产品 | MoE稀疏激活极致高效<br>原生多模态（图文输入→文本输出）<br>支持12种语言（数据量提升10倍） |
| 许可与可用性   | 仅限研究，禁止微调       | 开源可商用                       | 开源可商用                        | 开源可商用（最宽松版本）                                          |

LLaMA模型的发展历程展现出一条稳步创新的轨迹，旨在提升LLM的效率、性能和通用性。
- 从LLaMA1 开始，它引入了诸如使用RMSNorm进行输入归一化和更平滑的激活函数等基础性变革，后续的每个版本都在此基础上进行了改进。
- LLaMA2 通过 GQA 优化推理效率，改进了这种方法，为 LLaMA3 的更大改进奠定了基础。
- LLaMA3 通过将 GQA 扩展到更小的模型、采用词汇量更大的更高效的分词器、将上下文长度加倍以及显著增加训练数据，扩展了这些功能。

### LLaMA1

LLaMA1是 Meta 开发的一个高效开源语言模型，在继承 Transformer 架构的基础上，LLaMA1 进行了一系列关键的技术改进，包括`预归一化与RMSNorm`、`旋转位置编码(ROPE) ` 、`SwiGLU`激活函数等技术，这些改进显著提升了模型的性能和训练稳定性。

#### 预归一化与RMSNorm

> [The Evolution of Llama: From Llama 1 to Llama 3.1](https://zaai.ai/the-evolution-of-llama-from-llama-1-to-llama-3-1/)

受 GPT3 架构中训练稳定性改进的启发 [4]，Llama 1 也对每个 Transformer 子层的输入进行归一化，而不仅仅是对输出进行归一化，如下图所示：

![](/img/post/20251208/1.png)

此外，使用 RMSNorm 代替了传统的 LayerNorm 函数，通过归一化向量的均方根值，而不需要中心化或依赖均值。

![](/img/post/20251208/2.png)

RMSNorm之所以能取得更高的效率，是因为其作者证明了LayerNorm的优势源于重缩放不变性而非中心化不变性。这一发现使他们能够从归一化过程中移除均值计算，从而简化了流程，使其同样有效，并显著提高了效率。

#### 旋转位置编码(ROPE)

**位置嵌入**对于语言学习模型（LLM）至关重要，因为Transformer架构具有顺序不变性。这意味着即使两个句子使用了相同的词语，但顺序不同且含义不同，Transformer也会以相同的方式表示它们。例如，如果不应用位置嵌入，以下句子对于Transformer来说含义相同：

- Sentence 1: Llama 2 is better than Llama 1 
- Sentence 2: Llama 1 is better than Llama 2

原始论文（Attention Is All You Need）实现了绝对位置嵌入，该嵌入通过两个正弦函数（正弦和余弦）表示。序列中的每个位置都有一个唯一的位置嵌入，这些位置嵌入被加总到词嵌入中，从而确保包含相同词语的两个句子含义不同。

![](/img/post/20251208/3.png)

尽管它已经解决了Transformer模型顺序不变的问题，但它仍然创建彼此独立的嵌入位置。其结果是，两个位置之间的邻近性没有被建模。这意味着从模型的角度来看，位置1和2与位置1和500之间的相关性没有区别。我们知道实际情况并非如此，因为理论上，位置1和2中单词的相似度应该高于位置1和500中单词的相似度。

**旋转位置嵌入(RoPE)**可以解决这个问题，它通过旋转词嵌入来表示序列中每个位置，从而对单词的相对位置进行建模。我们沿用之前的例子：“Llama 2 is better than Llama 1”，并假设词嵌入现在是二维的。单词“better”将由原始二维向量基于其位置 m(4) 和常数 θ 旋转得到的二维向量表示，如下图所示：

![](/img/post/20251208/4.png)

这种方法可以让我们保持词语之间的相对距离，因为即使我们在原句中添加更多词语，两个向量之间的相似度也保持不变。例如，假设我们在句子“LLM Llama 2 比 Llama 1 好”中添加两个词，其中“better”和“than”的位置不同（4 和 5 → 6 和 7），但由于旋转角度相同，两个向量之间的相似度也保持不变（左图中向量之间的点积与右图中的点积相同）。

![](/img/post/20251208/5.png)

![RoPE embeddings](https://x.com/rasbt/status/1637803707793326081/photo/1)

![](/img/post/20251208/6.png)

#### SwiGLU 激活函数

**SwiGLU（Swish-Gated Linear Unit）**是一种结合非线性激活函数的门控单元激活机制，相比传统的 ReLU 函数，增强了激活函数的表达能力，使模型可以捕捉更复杂的模式。此外，SwiGLU 减少了梯度消失问题(ReLU 容易导致负值被切为0)，从而提升训练稳定性和模型性能。

**1. 基本定义**

给定输入向量 \( x \)，线性层生成两个投影：

- \( xW_1 \)
- \( xW_2 \)

SwiGLU 的定义如下：

\[
\text{SwiGLU}(x) = (xW_1) \otimes \text{swish}(xW_2)
\]

其中：

- \( \otimes \) 表示逐元素乘法（element-wise product）
- Swish 激活函数定义为：

\[
\text{swish}(z) = z \cdot \sigma(z)
\]

Sigmoid 为：

\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

**2. 详细公式展开**

\[
\text{SwiGLU}(x)
  = (xW_1) \cdot \big[(xW_2) \cdot \sigma(xW_2)\big]
\]


**3. 与其他 GLU 变体的对比**

| 激活名称       | 公式                              | 激活函数类型  |
| ---------- | ------------------------------- | ------- |
| **GLU** | \( (xW_1) \otimes \sigma(xW_2) \) | Sigmoid |
| **ReGLU** | \( (xW_1) \otimes \text{ReLU}(xW_2) \) | ReLU |
| **GEGLU** | \( (xW_1) \otimes \text{GELU}(xW_2) \) | GELU |
| **SwiGLU** | \( (xW_1) \otimes \text{swish}(xW_2) \) | Swish |

SwiGLU 在性能与计算效率上获得更好折中，是 LLaMA、PaLM 等模型的默认 MLP 激活。

![](/img/post/20251208/7.png)

SwiGLU 具有一个可训练参数 β，用于控制插值程度。如图 4 所示，随着 β 值的增加，其行为越来越类似于 ReLU（当 β=100 时，ReLU 和 SwiGLU 重叠）。

### LLaMA2

LLaMA2 的架构和 LLaMA1 没有太大的变化，只是多了`分组查询注意力(GQA)`。而且在上下文处理能力（将上下文长度从 2048 增加到 4096）、注意力机制优化、训练数据质量、性能表现、多语言支持和推理效率方面实现了全面改进，进一步缩小了开源模型与闭源大型模型之间的性能差距。

#### 分组查询注意力(GQA)

GQA是一种优化多头注意力机制的方法，将Q的头数分组，减少计算复杂度。传统的多头注意力机制为每个注意力头生成独立的 Q,K,V，而 GQA 将查询头分为少数几组，共享 Q。

传统注意力公式是:

![](/img/post/20251208/8.png)

在 GOA中: Q被分为G组，每组共享相同的Q

![](/img/post/20251208/9.png)

多查询注意力机制(MQA) 通过在注意力层中使用单个键值对但多个查询头，显著降低了内存需求。然而，这种方案可能会导致模型质量下降和训练不稳定，因此其他开源的层级模型（例如 T5）并未采用这种方法。

GQA 介于 MHA 和 MQA 之间，它将查询值分成 G 个组（GQA-G），每个组共享一个键值对。GQA-1 表示所有查询都聚合在一个组中，因此与 MQA 相同；而 GQA-H（H = 键值对数量）则等同于 MHA，其中每个查询都被视为一个组。这种方法将键值对的数量减少到每个查询组一个键值对。它减少了缓存的键值对的大小，从而减少了需要加载的数据量。与 MQA 相比，这种更为温和的缩减方式提高了推理速度，并在解码过程中降低了内存需求，同时保持了接近 MHA 的质量和几乎与 MQA 相同的速度。

### LLaMA3

LLaMA3 的架构和 LLaMA1、2 没有太大的变化。但是细节方面有些改变:
- 模型规模与参数:LLaMA3提供了不同规模的模型，包括8B(80亿参数)、70B(700亿参数)和 405B(4050 亿参数)等，以满足不同的应用需求。
- 超长上下文窗口:LLaMA3的上下文窗口长度达到了8k，显著提升了模型处理长文本和复杂上下文的能力。
- 广泛的语言覆盖:LLaMA3支持多种语言，增强了模型的多语言处理能力，适用于全球化的应用场景。
- 强化的安全措施:LLaMA3集成了 Llama Guard3等安全工具，提升了模型的安全性和对齐能力，减少了有害输出的可能性。

#### LLaMA 3.1

- 超长上下文窗口:128k，显著提升了模型处理长文本的能力。
- 多语言支持:支持包括英语、中文、西班牙语、法语、德语、日语、韩语和阿拉伯语在内的八种语@言，增强了模型的全球适用性。
- 高性能与高效训练:在超过 15 万亿个 token 上进行训练，使用超过 16,000 个 H100 GPU 进行优化，确保模型的高性能和高效能。
- 多模态能力:具体包括多模态编码器的预训练、视觉适配器训练和语音适配器训练。这些技术的核心目标是将不同模态的信息(图像、语音)与语言模型进行高效集成，使模型能够理解和生成多模态内容。

#### LLaMA 3.2

- 小型模型发布:推出了 1B和3B参数的轻量级模型，专为移动设备和边缘设备设计，支持在本地运行，满足低资源环境下的应用需求。
- 超长上下文处理:模型的上下文长度扩展至128k个token，显著提升了处理长文本和复杂上下文的能力。
- 模型剪枝与蒸馏:通过剪枝和知识蒸馏技术，优化了模型的性能，使轻量级模型在保持高效椎理的同时，具备较强的任务处理能力。

#### LLaMA 3.3

- 编码能力增强:模型在编码任务中表现优异，提供详细的错误处理、广泛的编程语言支持和结构化的代码反馈，助力开发者更高效地编写、调试和优化代码。
- 监督微调与强化学习:通过监督微调(SFT)和基于人类反馈的强化学习(RLHF)，模型更好地与人类对有用性和安全性的偏好保持一致，减少不当内容的生成。

### LLaMA4

