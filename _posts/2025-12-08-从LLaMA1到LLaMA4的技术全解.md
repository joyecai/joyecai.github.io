---
layout:     post
title:      从LLaMA1到LLaMA4的技术全解
subtitle:   LLaMA、架构、算法
date:       2025-12-08
author:     Jiayue Cai
header-img: img/post-bg-ai.jpg
catalog: true
tags:
    - LLM
    - AI
---

> Last updated on 2025-12-8... 

### 模型发布概览

> [From Llama 1 to Llama 4: Meta’s AI Evolution Unleashed](https://www.linkedin.com/pulse/from-llama-1-4-metas-ai-evolution-unleashed-dheeraj-kumar-k1vxc/)

| 版本           | LLaMA 1                  | LLaMA 2                          | LLaMA 3 / 3.1                     | LLaMA 4（2025年4月5日发布）                                      |
|----------------|--------------------------|----------------------------------|-----------------------------------|------------------------------------------------------------------|
| 发布日期       | 2023年2月                | 2023年7月                        | 2024年4月（3.1为7月更新）          | 2025年4月5日                                                     |
| 参数规模       | 7B、13B、33B、65B        | 7B、13B、70B（含Chat版）          | 8B、70B、405B                     | Scout：17B活跃/109B总<br>Maverick：17B活跃/400B总<br>Behemoth：288B活跃/约2T总（即将发布） |
| 训练数据量     | 1.4万亿 token（公开数据） | 2万亿 token（更高质量、多样）     | 15万亿 token（含大量代码+30+语言）  | 30万亿 token（首次加入Meta内部用户数据）                           |
| 上下文长度     | 未特别强调               | 4K token                         | 128K（部分变体支持1M）             | Scout：1000万 token<br>Maverick：100万 token                      |
| 主要基准表现   | 13B在多项任务超越GPT-3（175B） | 接近或超过同期闭源模型            | 大幅领先同级别开源模型，逼近GPT-4   | Maverick在多语言、编码任务超越GPT-4o<br>Scout长上下文表现极强      |
| 关键技术与特性 | 纯研究用途、高效证明概念  | 开源权重+商业许可、RLHF安全对齐   | 多语言（30+种）、超长上下文、驱动Meta全家桶产品 | MoE稀疏激活极致高效<br>原生多模态（图文输入→文本输出）<br>支持12种语言（数据量提升10倍） |
| 许可与可用性   | 仅限研究，禁止微调       | 开源可商用                       | 开源可商用                        | 开源可商用（最宽松版本）                                          |


### LLaMA1

LLaMA1是 Meta 开发的一个高效开源语言模型，在继承 Transformer 架构的基础上，LLaMA1 进行了一系列关键的技术改进，包括`预归一化与RMSNorm`、`旋转位置编码(ROPE) ` 、`SwiGLU`激活函数等技术，这些改进显著提升了模型的性能和训练稳定性。

#### 预归一化与RMSNorm

> [The Evolution of Llama: From Llama 1 to Llama 3.1](https://zaai.ai/the-evolution-of-llama-from-llama-1-to-llama-3-1/)

受 GPT3 架构中训练稳定性改进的启发 [4]，Llama 1 也对每个 Transformer 子层的输入进行归一化，而不仅仅是对输出进行归一化，如下图所示：

![](/img/post/20251208/1.png)

此外，使用 RMSNorm 代替了传统的 LayerNorm 函数，通过归一化向量的均方根值，而不需要中心化或依赖均值。

![](/img/post/20251208/2.png)

RMSNorm之所以能取得更高的效率，是因为其作者证明了LayerNorm的优势源于重缩放不变性而非中心化不变性。这一发现使他们能够从归一化过程中移除均值计算，从而简化了流程，使其同样有效，并显著提高了效率。

#### 旋转位置编码(ROPE)

**位置嵌入**对于语言学习模型（LLM）至关重要，因为Transformer架构具有顺序不变性。这意味着即使两个句子使用了相同的词语，但顺序不同且含义不同，Transformer也会以相同的方式表示它们。例如，如果不应用位置嵌入，以下句子对于Transformer来说含义相同：

- Sentence 1: Llama 2 is better than Llama 1 
- Sentence 2: Llama 1 is better than Llama 2

原始论文（Attention Is All You Need）实现了绝对位置嵌入，该嵌入通过两个正弦函数（正弦和余弦）表示。序列中的每个位置都有一个唯一的位置嵌入，这些位置嵌入被加总到词嵌入中，从而确保包含相同词语的两个句子含义不同。

![](/img/post/20251208/3.png)

尽管它已经解决了Transformer模型顺序不变的问题，但它仍然创建彼此独立的嵌入位置。其结果是，两个位置之间的邻近性没有被建模。这意味着从模型的角度来看，位置1和2与位置1和500之间的相关性没有区别。我们知道实际情况并非如此，因为理论上，位置1和2中单词的相似度应该高于位置1和500中单词的相似度。

**旋转位置嵌入(RoPE)**可以解决这个问题，它通过旋转词嵌入来表示序列中每个位置，从而对单词的相对位置进行建模。我们沿用之前的例子：“Llama 2 is better than Llama 1”，并假设词嵌入现在是二维的。单词“better”将由原始二维向量基于其位置 m(4) 和常数 θ 旋转得到的二维向量表示，如下图所示：

![](/img/post/20251208/4.png)

这种方法可以让我们保持词语之间的相对距离，因为即使我们在原句中添加更多词语，两个向量之间的相似度也保持不变。例如，假设我们在句子“LLM Llama 2 比 Llama 1 好”中添加两个词，其中“好”和“比”的位置不同（4 和 5 → 6 和 7），但由于旋转角度相同，两个向量之间的相似度也保持不变（左图中向量之间的点积与右图中的点积相同）。

![](/img/post/20251208/5.png)

#### SwiGLU 

SwiGLU 是一种结合非线性激活函数的门控单元激活机制，相比传统的 ReLU 函数，增强了激活函数的表达能力，使模型可以捕捉更复杂的模式。此外，SwiGLU 减少了梯度消失问题(ReLU 容易导致负值被切为0)，从而提升训练稳定性和模型性能。

$$
\text{SwiGLU}(x, W, V, b, c) = \text{Swish}_\beta(xW + b) \odot (xV + c)
$$

其中：
- $\odot$ 表示逐元素相乘（Hadamard product）
- $\text{Swish}_\beta(x) = x \cdot \sigma(\beta x)$，$\beta$ 为可学习参数或固定为 1
- 当 $\beta=1$ 时（最常见用法）：

$$
\boxed{\text{SwiGLU}(x) = (xW + b) \cdot \sigma(xW + b) \odot (xV + c)}
$$

或进一步简化（实际工程中最常见的写法）：

$$
\boxed{\text{SwiGLU}(x) = (xW) \odot \sigma(xW) \odot (xV)}
$$

（省略偏置，$\beta=1$，这是 LLaMA、DeepSeek、Qwen 等模型中实际采用的形式）

![](/img/post/20251208/6.png)

SwiGLU 具有一个可训练参数 β，用于控制插值程度。如图 4 所示，随着 β 值的增加，其行为越来越类似于 ReLU（当 β=100 时，ReLU 和 SwiGLU 重叠）。

### LLaMA2


### LLaMA3



### LLaMA4