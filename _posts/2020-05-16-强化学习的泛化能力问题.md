---
layout:     post
title:      强化学习的泛化能力问题
subtitle:   环境随机化、鲁棒性优化、启发式正则、sim2real
date:       2020-05-16
author:     Jiayue Cai
header-img: img/post-bg-ai.jpg
catalog: true
tags:
    - Reinforcement Learning
---


> Last updated on 2020-05-17... 

> 本篇整理自[《炼丹感悟：On the Generalization of RL》](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247503049&idx=2&sn=1164dcccb7cede8d75a743f9739b1c0f&chksm=96ea1349a19d9a5f792e91088096c2ecbb2dac7a00b6c373eb9848bd87234c97996cbaf89a5f&mpshare=1&scene=23&srcid=0212S6vjRE8w0vvZdvG3o1Zo&sharer_sharetime=1581473512553&sharer_shareid=cc983be31429dfbd5199d63f0d94b825#rd)

目前大部分强化学习论文使用的主要基准任务实际上都是偏弱的，比如MuJoCo或者Atari，更不用说前两年`MARL`用的[multiagent-particle-envs](https://github.com/openai/multiagent-particle-envs)。
在偏弱的实验环境里，模型跑出来的结果看起来尚可，模型的许多问题暴露得不明显就不会引起大家的广泛关注，因此相关的研究也就会比较少。 
而强化学习的泛化能力问题，就是这些问题之一。

> 往期相关传送门：[《强化学习总览》](https://coladrill.github.io/2018/12/02/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%80%BB%E8%A7%88/)、[《深度强化学习综述》](https://coladrill.github.io/2018/10/21/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/)、[《强化学习在推荐系统中的应用》](https://coladrill.github.io/2018/11/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9C%A8%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/)、[《推荐系统的发展与简单回顾》](https://coladrill.github.io/2019/12/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8F%91%E5%B1%95%E4%B8%8E%E7%AE%80%E5%8D%95%E5%9B%9E%E9%A1%BE/)

### 理论与实践的差异

**理论层面**，由于强化学习本身的理论建模中不存在泛化问题，目前学术界的研究大部分都是经验性的工作，理论性文章很少。 

**实践层面**，最大的困难来自于当前`model-free RL`模型高方差的问题，工业界的场景远比学术界所常用的MuJoCo和Atari任务复杂

因此，在 MuJoCo、Atari和随机迷宫之类的环境中，如果你做出了某种提升，可能提升的幅度还不如代码层面优化换`random seed`或者`reward scaling`来的明显，今年ICLR就有一篇讲RL的`code-level optimization`的文章工作被接受，足以说明现在的RL研究者对可复现性（reproducibility）的殷切期盼。

基本上大多数研究 RL generalization 或 robustness 的文章里都会涉及到，举一些代表性的例子： 

|    **过拟合于状态特征**     | `ICLR 2020`[Observational Overfitting in Reinforcement Learning](https://arxiv.org/abs/1912.02975)|
|    **动作空间的随机性**     | `ICML 2019`[Action Robust Reinforcement Learning and Applications in Continuous Control](https://arxiv.org/abs/1901.09184)|
|  **连续控制任务上的过拟合** | `arXiv 2018`[Dissection of Overfitting and Generalization in Continuous Reinforcement Learning](https://arxiv.org/abs/1806.07937)|
|    **死记硬背型过拟合**     | `arXiv 2018`[A Study on Overfitting in Deep Reinforcement Learning](https://arxiv.org/abs/1804.06893)|
|       **环境动态**           | `arXiv 2016`[Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes](https://arxiv.org/abs/1604.02080)|

> [Quantifying Generalization in Reinforcement Learning](https://openai.com/blog/quantifying-generalization-in-reinforcement-learning/)

### 环境随机化 environmental randomization 

在目前的工业应用中，环境加随机化可能是使用最广泛的解决方案，在2018年 [《Assessing Generalization in Deep Reinforcement Learning》](https://arxiv.org/abs/1810.12282) 一文中，
作者基于若干组 MuJoCo 的实验，声称环境加随机化是目前为止提升泛化能力最有效的方法。

加入环境随机化后，所存在的问题：
- 环境的复杂度上升：环境的数量会随随机变量数量的上升而指数上升
- 训练的复杂度上升：模型训练达到收敛所需的样本量实际上显著上升
- 方差急剧增大：加入环境随机化后，模型表现无论是在训练环境还是测试环境中，表现出的方差要大于在单一环境上训练（高出若干个数量级）。

|[Solving Rubik’s Cube with a Robot Hand](https://openai.com/blog/solving-rubiks-cube/)|

### 鲁棒性优化 robust optimization

在早期 RL 研究者们还聚焦于各种 MDP 数学意义上的tractability的时候，就已经有一些工作研究在不完美、或者包含不确定性的环境动态的基础上进行优化，
这种优化一般被称作`robust MDP`。

|[Risk sensitive path integral control](https://event.cwi.nl/uai2010/papers/UAI2010_0266.pdf)|
|[Robust control of markov decision processes with uncertain transition matrices](http://robotics.eecs.berkeley.edu/~elghaoui/pdffiles/rmdp_erl.pdf)|
|[Robust markov decision processes](https://www.semanticscholar.org/paper/Robust-Markov-Decision-Processes-Wiesemann-Kuhn/9eae0c6ca4a52fc5e6b6f9eb111ab6fdbecdf9a6)|
|[Risk-sensitive reinforcement learning](https://arxiv.org/abs/1311.2097)、[Risk-sensitive and robust decision-making: a cvar optimization approach](https://arxiv.org/abs/1506.02188)|

进入深度学习时代以后，最早的代表性工作是 [RARL模型](https://arxiv.org/abs/1703.02702) ，
在训练中引入了 zero-sum Markov game 来提升鲁棒性，优化目标为总回报的CVaR，后来的很多文章都沿用了RARL的框架。

|[Wasserstein Robust Reinforcement Learning](https://arxiv.org/abs/1907.13196)|
|[Robust Reinforcement Learning for Continuous Control with Model Misspecification](https://openreview.net/forum?id=HJgC60EtwB)|
|[Distributionally Robust Reinforcement Learning](https://openreview.net/pdf?id=r1xfz_93oN)|
|[EPOpt: Learning Robust Neural Network Policies Using Model Ensembles](https://arxiv.org/abs/1610.01283)|
|[Robust Reinforcement Learning via Adversarial Training with Langevin Dynamics](https://openreview.net/forum?id=BJl7mxBYvB)|

再后来，引入了`对抗样本`，产生了新的问题：
- 思路可以，但是对复杂任务完全没有效果。
- 将对抗样本引入DDPG训练，采用经对抗扰动后最差的policy做off-policy训练，因此导致收敛速度非常慢。

|[Certifying Some Distributional Robustness with Principled Adversarial Training](https://arxiv.org/abs/1710.10571)|
|[Robust Deep Reinforcement Learning with Adversarial Attacks](https://arxiv.org/abs/1712.03632)|
|[Adversarial Attacks on Neural Network Policies](https://arxiv.org/abs/1702.02284)|

> 最新综述：[Challenges and Countermeasures for Adversarial Attacks on Deep Reinforcement Learning](https://arxiv.org/abs/2001.09684)

### 启发式正则 heuristic regularization

在传统机器学习中人们就常用各种正则项来抑制过拟合，最典型的应该是 L2 正则，而近些年 RL 领域中也有若干 paper 研究正则项的影响，
比如今年 [《Regularization Matters in Policy Optimization》](https://openreview.net/forum?id=B1lqDertwr) 的作者称 L2 可以起到比 entropy regularization 更好的效果，
DL 里常用的 dropout 可以为 off-policy 训练带来一些正面的效果（虽然最后被 reject 掉了）。

而 [《Generalization and Regularization in DQN》](https://arxiv.org/abs/1810.00123) 中作者称，模型训练到后期，训练环境上模型能力的上升同时也意味着泛化能力的下降，
而 L2 正则可以找到一个较好的平衡点，类似于 supervised learning 里面 L2 与 early stop 的等价关系。

### 仅限机器人训练任务的sim2real

在机器人训练任务中，因为机器人机械臂有使用寿命的限制，目前常用的一种方式是在物理仿真模拟环境中训练，模型收敛后部署到现实世界中，
然而模拟器不大可能建模出现实世界中所有的变量，实际上模拟器中表现良好的模型，在现实世界的表现会有所下降。

目前主要考虑两种解决方案：
- 在训练期在模拟器中加入随机化
- 将从模拟器到现实的过程理解为是一个迁移学习的问题

|[Adapting Deep Visuomotor Representations with Weak Pairwise Constraints](https://arxiv.org/abs/1511.07111)|
|[Sim-to-Real Transfer of Robotic Control with Dynamics Randomization](https://arxiv.org/abs/1710.06537)|

### 其他

目前学术界对 RL 泛化问题的研究实际上很难完全分类，因为目前鲜有能构成体系的研究工作，提出的各种解决方案颇有东一榔头西一棒槌之感，下面列两篇无法分类到以上任何类别的工作：

|[Improving Generalization in Meta Reinforcement Learning using Neural Objectives](https://openreview.net/forum?id=S1evHerYPr)|
|[On the Generalization Gap in Reparameterizable Reinforcement Learning](https://arxiv.org/abs/1905.12654)|

第二篇是 RL 泛化问题上为数不多的理论文章之一，用有限样本做分析的，值得一读，缺点是只能用于on-policy和reparameterizable state的情况。



